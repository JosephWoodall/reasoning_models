services:
  # Main training service
  reasoning-models:
    build:
      context: .
      dockerfile: src/training_pipeline/Dockerfile
    container_name: reasoning-models-training
    
    # GPU support using deploy syntax (working with system Docker daemon)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    volumes:
      # Mount the entire project directory
      - .:/workspace
      # Mount a persistent volume for outputs
      - ./output:/workspace/output
      # Mount text cache
      - ./text_cache:/workspace/text_cache
    
    working_dir: /workspace
    
    # Run the training script
    command: python src/training_pipeline/train_model.py
    
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - PYTHONPATH=/workspace/src
      - CUDA_VISIBLE_DEVICES=0
    
    # Restart policy - only restart if there's a failure
    restart: "no"
