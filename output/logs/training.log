2025-08-14 15:17:33,587 - INFO - Starting transformer training...
2025-08-14 15:17:33,588 - INFO - Configuration: {
  "model": {
    "d_model": 384,
    "num_heads": 6,
    "num_layers": 6,
    "d_ff": 1536,
    "max_len": 2048,
    "dropout": 0.2,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 10,
    "epochs": 20,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 1,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "src/data/text_cache",
    "sequence_length": 512,
    "min_text_length": 500,
    "train_split": 0.95,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "auto",
    "num_workers": 2,
    "pin_memory": true
  },
  "evaluation": {
    "eval_frequency": 2,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 2000
  }
}
2025-08-14 15:17:33,754 - INFO - Using device: cuda
2025-08-14 15:17:33,754 - INFO - Loading text data from /home/joseph_woodall/workspace/reasoning_models/src/training_pipeline/../../src/data/text_cache...
2025-08-14 15:17:33,962 - INFO - Progress: 99/99 files (100.0%)
2025-08-14 15:17:33,962 - INFO - Loaded batch of 99 texts (Total: 99 texts, 60.9M chars)
2025-08-14 15:17:33,962 - INFO - Completed loading 99 text files (60.9M characters)
2025-08-14 15:17:33,962 - INFO - Train texts: 94, Validation texts: 5
2025-08-14 15:17:33,962 - INFO - Building vocabulary with subword tokenization...
2025-08-14 15:18:07,880 - INFO - Built vocabulary with 9,616 tokens
