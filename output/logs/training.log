2025-08-25 16:59:54,035 - INFO - Starting transformer training (BPE tokenizer)...
2025-08-25 16:59:54,035 - INFO - Configuration: {
  "model": {
    "d_model": 512,
    "num_heads": 8,
    "num_layers": 6,
    "d_ff": 2048,
    "max_len": 512,
    "dropout": 0.2,
    "src_vocab_size": 32000,
    "tgt_vocab_size": 32000
  },
  "training": {
    "batch_size": 32,
    "epochs": 10,
    "learning_rate": 0.0003,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 1000,
      "step_factor": 0.5,
      "step_size": 5
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 1,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "src/data/text_cache",
    "sequence_length": 256,
    "min_text_length": 500,
    "train_split": 0.95,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    },
    "load_batch_size": 1000
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm",
    "tokenizer_path": "output/checkpoints/tokenizer.json"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.9,
    "top_k": 40,
    "top_p": 0.95,
    "num_samples": 5,
    "sample_prompt": "The meaning of life is",
    "sample_display_length": 200
  },
  "hardware": {
    "device": "auto",
    "num_workers": 2,
    "pin_memory": true
  },
  "evaluation": {
    "eval_frequency": 2,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 2000
  }
}
2025-08-25 16:59:54,095 - INFO - Using device: cuda
2025-08-25 16:59:54,095 - INFO - Loading text data from /home/joseph_woodall/workspace/reasoning_models/src/training_pipeline/../../src/data/text_cache...
2025-08-25 16:59:55,427 - INFO - Progress: 669/669 files (100.0%)
2025-08-25 16:59:55,427 - INFO - Loaded batch of 669 texts (Total: 669 texts, 364.7M chars)
2025-08-25 16:59:55,427 - INFO - Completed loading 669 text files (364.7M characters)
2025-08-25 16:59:55,428 - INFO - Train texts: 635, Validation texts: 34
2025-08-25 16:59:55,428 - INFO - Loading existing tokenizer from /home/joseph_woodall/workspace/reasoning_models/src/training_pipeline/../../output/checkpoints/tokenizer.json
2025-08-25 17:03:23,680 - INFO - Model parameters: 93,294,848 total, 93,294,848 trainable
2025-08-25 17:03:26,181 - INFO - Batch 0/23390, Loss: 10.5775, LR: 0.000001
2025-08-25 17:03:27,737 - INFO - Batch 10/23390, Loss: 9.5182, LR: 0.000015
2025-08-25 17:03:29,304 - INFO - Batch 20/23390, Loss: 9.2209, LR: 0.000029
2025-08-25 17:03:30,861 - INFO - Batch 30/23390, Loss: 8.9762, LR: 0.000043
2025-08-25 17:03:32,461 - INFO - Batch 40/23390, Loss: 8.6734, LR: 0.000057
2025-08-25 17:03:34,016 - INFO - Batch 50/23390, Loss: 8.4369, LR: 0.000071
2025-08-25 17:03:35,512 - INFO - Batch 60/23390, Loss: 8.0839, LR: 0.000085
