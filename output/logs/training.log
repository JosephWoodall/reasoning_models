2025-08-04 11:28:26,827 - INFO - Starting transformer training...
2025-08-04 11:28:26,828 - INFO - Configuration: {
  "model": {
    "d_model": 512,
    "num_heads": 8,
    "num_layers": 6,
    "d_ff": 2048,
    "max_len": 1024,
    "dropout": 0.1,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 32,
    "epochs": 100,
    "learning_rate": 0.0001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 10,
    "log_frequency": 100
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 512,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "auto",
    "num_workers": 4,
    "pin_memory": true
  },
  "evaluation": {
    "eval_frequency": 5,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-04 11:28:26,984 - INFO - Using device: cuda
2025-08-04 11:28:26,985 - INFO - Train texts: 0, Validation texts: 1
2025-08-04 11:28:44,724 - INFO - Starting transformer training...
2025-08-04 11:28:44,724 - INFO - Configuration: {
  "model": {
    "d_model": 512,
    "num_heads": 8,
    "num_layers": 6,
    "d_ff": 2048,
    "max_len": 1024,
    "dropout": 0.1,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 32,
    "epochs": 100,
    "learning_rate": 0.0001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 10,
    "log_frequency": 100
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 512,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "auto",
    "num_workers": 4,
    "pin_memory": true
  },
  "evaluation": {
    "eval_frequency": 5,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-04 11:28:44,778 - INFO - Using device: cuda
2025-08-04 11:28:44,779 - INFO - Train texts: 1, Validation texts: 1
2025-08-04 11:28:45,263 - INFO - Model parameters: 49,659,418 total, 49,659,418 trainable
2025-08-04 11:29:36,324 - INFO - Starting transformer training...
2025-08-04 11:29:36,324 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 1024,
    "dropout": 0.1,
    "src_vocab_size": 5000,
    "tgt_vocab_size": 5000
  },
  "training": {
    "batch_size": 4,
    "epochs": 5,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 100,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 2,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 128,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "cpu",
    "num_workers": 2,
    "pin_memory": false
  },
  "evaluation": {
    "eval_frequency": 5,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-04 11:29:36,325 - INFO - Using device: cpu
2025-08-04 11:29:36,326 - INFO - Train texts: 1, Validation texts: 1
2025-08-04 11:29:36,362 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-04 11:29:37,215 - INFO - Batch 0/80, Loss: 8.3858, LR: 0.000088
2025-08-04 11:29:37,648 - INFO - Batch 10/80, Loss: 7.4111, LR: 0.000972
2025-08-04 11:29:38,037 - INFO - Batch 20/80, Loss: 6.5957, LR: 0.001856
2025-08-04 11:29:38,446 - INFO - Batch 30/80, Loss: 6.4169, LR: 0.002740
2025-08-04 11:29:38,826 - INFO - Batch 40/80, Loss: 6.5445, LR: 0.003624
2025-08-04 11:29:39,198 - INFO - Batch 50/80, Loss: 6.2436, LR: 0.004508
2025-08-04 11:29:39,587 - INFO - Batch 60/80, Loss: 6.2511, LR: 0.005392
2025-08-04 11:29:39,970 - INFO - Batch 70/80, Loss: 5.5475, LR: 0.006276
2025-08-04 11:29:40,901 - INFO - Epoch 0/5:
2025-08-04 11:29:40,901 - INFO -   Train Loss: 6.5989
2025-08-04 11:29:40,901 - INFO -   Val Loss: 5.5300
2025-08-04 11:29:40,902 - INFO -   Perplexity: 252.14
2025-08-04 11:29:40,940 - INFO - Saved checkpoint at epoch 0
2025-08-04 11:29:41,206 - INFO - Sample: the bottom his / a moment , and the edges . the trademark . the birds and underwear . our happy the __________ is a pages . a start cycle bird to ' the borders from for it set hop and be off , and how...
2025-08-04 11:29:42,044 - INFO - Saved checkpoint at epoch 0
2025-08-04 11:29:42,044 - INFO - Epoch 0 completed in 4.96s
2025-08-04 11:29:42,163 - INFO - Batch 0/80, Loss: 5.8722, LR: 0.007159
2025-08-04 11:29:42,552 - INFO - Batch 10/80, Loss: 5.6442, LR: 0.008043
2025-08-04 11:29:42,913 - INFO - Batch 20/80, Loss: 5.1810, LR: 0.008795
2025-08-04 11:29:43,280 - INFO - Batch 30/80, Loss: 5.7597, LR: 0.008389
2025-08-04 11:29:43,652 - INFO - Batch 40/80, Loss: 5.7197, LR: 0.008035
2025-08-04 11:29:44,023 - INFO - Batch 50/80, Loss: 5.1805, LR: 0.007723
2025-08-04 11:29:44,404 - INFO - Batch 60/80, Loss: 5.1411, LR: 0.007444
2025-08-04 11:29:44,763 - INFO - Batch 70/80, Loss: 5.1676, LR: 0.007193
2025-08-04 11:29:45,124 - INFO - Epoch 1 completed in 3.08s
2025-08-04 11:29:45,211 - INFO - Batch 0/80, Loss: 4.7935, LR: 0.006966
2025-08-04 11:29:45,574 - INFO - Batch 10/80, Loss: 4.8596, LR: 0.006759
2025-08-04 11:29:45,962 - INFO - Batch 20/80, Loss: 4.8292, LR: 0.006570
2025-08-04 11:29:46,349 - INFO - Batch 30/80, Loss: 5.2018, LR: 0.006396
2025-08-04 11:29:46,722 - INFO - Batch 40/80, Loss: 4.6570, LR: 0.006234
2025-08-04 11:29:47,089 - INFO - Batch 50/80, Loss: 4.2525, LR: 0.006085
2025-08-04 11:29:47,450 - INFO - Batch 60/80, Loss: 4.6313, LR: 0.005946
2025-08-04 11:29:47,806 - INFO - Batch 70/80, Loss: 4.8415, LR: 0.005816
2025-08-04 11:29:48,276 - INFO - Saved checkpoint at epoch 2
2025-08-04 11:29:48,276 - INFO - Epoch 2 completed in 3.15s
2025-08-04 11:29:48,355 - INFO - Batch 0/80, Loss: 4.4453, LR: 0.005694
2025-08-04 11:29:48,720 - INFO - Batch 10/80, Loss: 3.9193, LR: 0.005579
2025-08-04 11:29:49,084 - INFO - Batch 20/80, Loss: 4.2601, LR: 0.005471
2025-08-04 11:29:49,436 - INFO - Batch 30/80, Loss: 3.7360, LR: 0.005369
2025-08-04 11:29:49,830 - INFO - Batch 40/80, Loss: 4.6225, LR: 0.005273
2025-08-04 11:29:50,183 - INFO - Batch 50/80, Loss: 3.7020, LR: 0.005181
2025-08-04 11:29:50,538 - INFO - Batch 60/80, Loss: 3.8855, LR: 0.005095
2025-08-04 11:29:50,891 - INFO - Batch 70/80, Loss: 4.0274, LR: 0.005012
2025-08-04 11:29:51,208 - INFO - Epoch 3 completed in 2.93s
2025-08-04 11:29:51,288 - INFO - Batch 0/80, Loss: 3.8217, LR: 0.004933
2025-08-04 11:29:51,642 - INFO - Batch 10/80, Loss: 3.4139, LR: 0.004858
2025-08-04 11:29:52,017 - INFO - Batch 20/80, Loss: 3.3500, LR: 0.004786
2025-08-04 11:29:52,377 - INFO - Batch 30/80, Loss: 3.8696, LR: 0.004718
2025-08-04 11:29:52,723 - INFO - Batch 40/80, Loss: 3.7260, LR: 0.004652
2025-08-04 11:29:53,077 - INFO - Batch 50/80, Loss: 3.9473, LR: 0.004589
2025-08-04 11:29:53,434 - INFO - Batch 60/80, Loss: 3.4008, LR: 0.004528
2025-08-04 11:29:53,791 - INFO - Batch 70/80, Loss: 3.8170, LR: 0.004470
2025-08-04 11:29:54,219 - INFO - Saved checkpoint at epoch 4
2025-08-04 11:29:54,220 - INFO - Epoch 4 completed in 3.01s
2025-08-04 11:29:54,220 - INFO - Training completed in 17.13s
2025-08-04 11:29:54,327 - INFO - Saved checkpoint at epoch 5
2025-08-04 11:29:54,327 - INFO - Training finished!
2025-08-04 11:31:02,228 - INFO - Starting transformer training...
2025-08-04 11:31:02,228 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 1024,
    "dropout": 0.1,
    "src_vocab_size": 5000,
    "tgt_vocab_size": 5000
  },
  "training": {
    "batch_size": 4,
    "epochs": 5,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 100,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 2,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 128,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "cpu",
    "num_workers": 2,
    "pin_memory": false
  },
  "evaluation": {
    "eval_frequency": 5,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-04 11:31:02,228 - INFO - Using device: cpu
2025-08-04 11:31:02,229 - INFO - Train texts: 1, Validation texts: 1
2025-08-04 11:31:02,264 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-04 11:31:03,068 - INFO - Batch 0/80, Loss: 8.3858, LR: 0.000088
2025-08-04 11:31:03,449 - INFO - Batch 10/80, Loss: 7.4111, LR: 0.000972
2025-08-04 11:31:03,828 - INFO - Batch 20/80, Loss: 6.5957, LR: 0.001856
2025-08-04 11:31:04,217 - INFO - Batch 30/80, Loss: 6.4169, LR: 0.002740
2025-08-04 11:31:04,633 - INFO - Batch 40/80, Loss: 6.5445, LR: 0.003624
2025-08-04 11:31:05,023 - INFO - Batch 50/80, Loss: 6.2436, LR: 0.004508
2025-08-04 11:31:05,405 - INFO - Batch 60/80, Loss: 6.2511, LR: 0.005392
2025-08-04 11:31:05,808 - INFO - Batch 70/80, Loss: 5.5475, LR: 0.006276
2025-08-04 11:31:06,763 - INFO - Epoch 0/5:
2025-08-04 11:31:06,764 - INFO -   Train Loss: 6.5989
2025-08-04 11:31:06,764 - INFO -   Val Loss: 5.5300
2025-08-04 11:31:06,764 - INFO -   Perplexity: 252.14
2025-08-04 11:31:06,965 - INFO - Saved checkpoint at epoch 0
2025-08-04 11:31:07,360 - INFO - Sample: the bottom his / a moment , and the edges . the trademark . the birds and underwear . our happy the __________ is a pages . a start cycle bird to ' the borders from for it set hop and be off , and how...
2025-08-04 11:31:07,561 - INFO - Saved checkpoint at epoch 0
2025-08-04 11:31:07,562 - INFO - Epoch 0 completed in 4.61s
2025-08-04 11:31:07,701 - INFO - Batch 0/80, Loss: 5.8722, LR: 0.007159
2025-08-04 11:31:08,061 - INFO - Batch 10/80, Loss: 5.6442, LR: 0.008043
2025-08-04 11:31:08,434 - INFO - Batch 20/80, Loss: 5.1810, LR: 0.008795
2025-08-04 11:31:08,799 - INFO - Batch 30/80, Loss: 5.7597, LR: 0.008389
2025-08-04 11:31:09,178 - INFO - Batch 40/80, Loss: 5.7197, LR: 0.008035
2025-08-04 11:31:09,534 - INFO - Batch 50/80, Loss: 5.1805, LR: 0.007723
2025-08-04 11:31:09,946 - INFO - Batch 60/80, Loss: 5.1411, LR: 0.007444
2025-08-04 11:31:10,370 - INFO - Batch 70/80, Loss: 5.1676, LR: 0.007193
2025-08-04 11:31:10,691 - INFO - Epoch 1 completed in 3.13s
2025-08-04 11:31:10,778 - INFO - Batch 0/80, Loss: 4.7935, LR: 0.006966
2025-08-04 11:31:11,127 - INFO - Batch 10/80, Loss: 4.8596, LR: 0.006759
2025-08-04 11:31:11,476 - INFO - Batch 20/80, Loss: 4.8292, LR: 0.006570
2025-08-04 11:31:11,856 - INFO - Batch 30/80, Loss: 5.2018, LR: 0.006396
2025-08-04 11:31:12,199 - INFO - Batch 40/80, Loss: 4.6570, LR: 0.006234
2025-08-04 11:31:12,542 - INFO - Batch 50/80, Loss: 4.2525, LR: 0.006085
2025-08-04 11:31:12,937 - INFO - Batch 60/80, Loss: 4.6313, LR: 0.005946
2025-08-04 11:31:13,283 - INFO - Batch 70/80, Loss: 4.8415, LR: 0.005816
2025-08-04 11:31:14,719 - INFO - Saved checkpoint at epoch 2
2025-08-04 11:31:14,720 - INFO - Epoch 2 completed in 4.03s
2025-08-04 11:31:14,826 - INFO - Batch 0/80, Loss: 4.4453, LR: 0.005694
2025-08-04 11:31:15,187 - INFO - Batch 10/80, Loss: 3.9193, LR: 0.005579
2025-08-04 11:31:15,541 - INFO - Batch 20/80, Loss: 4.2601, LR: 0.005471
2025-08-04 11:31:15,881 - INFO - Batch 30/80, Loss: 3.7360, LR: 0.005369
2025-08-04 11:31:16,262 - INFO - Batch 40/80, Loss: 4.6225, LR: 0.005273
2025-08-04 11:31:16,631 - INFO - Batch 50/80, Loss: 3.7020, LR: 0.005181
2025-08-04 11:31:16,987 - INFO - Batch 60/80, Loss: 3.8855, LR: 0.005095
2025-08-04 11:31:17,343 - INFO - Batch 70/80, Loss: 4.0274, LR: 0.005012
2025-08-04 11:31:17,672 - INFO - Epoch 3 completed in 2.95s
2025-08-04 11:31:17,765 - INFO - Batch 0/80, Loss: 3.8217, LR: 0.004933
2025-08-04 11:31:18,147 - INFO - Batch 10/80, Loss: 3.4139, LR: 0.004858
2025-08-04 11:31:18,558 - INFO - Batch 20/80, Loss: 3.3500, LR: 0.004786
2025-08-04 11:31:18,944 - INFO - Batch 30/80, Loss: 3.8696, LR: 0.004718
2025-08-04 11:31:19,327 - INFO - Batch 40/80, Loss: 3.7260, LR: 0.004652
2025-08-04 11:31:19,686 - INFO - Batch 50/80, Loss: 3.9473, LR: 0.004589
2025-08-04 11:31:20,056 - INFO - Batch 60/80, Loss: 3.4008, LR: 0.004528
2025-08-04 11:31:20,405 - INFO - Batch 70/80, Loss: 3.8170, LR: 0.004470
2025-08-04 11:31:22,728 - INFO - Saved checkpoint at epoch 4
2025-08-04 11:31:22,729 - INFO - Epoch 4 completed in 5.06s
2025-08-04 11:31:22,729 - INFO - Training completed in 19.78s
2025-08-04 11:31:23,245 - INFO - Saved checkpoint at epoch 5
2025-08-04 11:31:23,245 - INFO - Training finished!
2025-08-04 11:34:30,806 - INFO - Starting transformer training...
2025-08-04 11:34:30,807 - INFO - Configuration: {
  "model": {
    "d_model": 512,
    "num_heads": 8,
    "num_layers": 6,
    "d_ff": 2048,
    "max_len": 1024,
    "dropout": 0.1,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 32,
    "epochs": 100,
    "learning_rate": 0.0001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 10,
    "log_frequency": 100
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 512,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "auto",
    "num_workers": 4,
    "pin_memory": true
  },
  "evaluation": {
    "eval_frequency": 5,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-04 11:34:30,859 - INFO - Using device: cuda
2025-08-04 11:34:30,860 - INFO - Train texts: 1, Validation texts: 1
2025-08-04 11:34:31,263 - INFO - Model parameters: 49,659,418 total, 49,659,418 trainable
2025-08-04 11:35:16,017 - INFO - Starting transformer training...
2025-08-04 11:35:16,017 - INFO - Configuration: {
  "model": {
    "d_model": 512,
    "num_heads": 8,
    "num_layers": 6,
    "d_ff": 2048,
    "max_len": 1024,
    "dropout": 0.1,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 4,
    "epochs": 3,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 1,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 512,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "cpu",
    "num_workers": 2,
    "pin_memory": false
  },
  "evaluation": {
    "eval_frequency": 1,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-04 11:35:16,017 - INFO - Using device: cpu
2025-08-04 11:35:16,017 - INFO - Train texts: 1, Validation texts: 1
2025-08-04 11:35:16,274 - INFO - Model parameters: 49,659,418 total, 49,659,418 trainable
2025-08-04 11:35:19,194 - INFO - Batch 0/20, Loss: 8.2700, LR: 0.000000
2025-08-04 11:35:39,524 - INFO - Batch 10/20, Loss: 7.6735, LR: 0.000002
2025-08-04 11:43:24,873 - INFO - Starting transformer training...
2025-08-04 11:43:24,874 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 256,
    "dropout": 0.1,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 4,
    "epochs": 3,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 1,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 512,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "cpu",
    "num_workers": 2,
    "pin_memory": false
  },
  "evaluation": {
    "eval_frequency": 1,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-04 11:43:24,874 - INFO - Using device: cpu
2025-08-04 11:43:24,874 - INFO - Train texts: 1, Validation texts: 1
2025-08-04 11:43:24,912 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-04 11:43:25,975 - INFO - Batch 0/20, Loss: 8.3716, LR: 0.000000
2025-08-04 11:43:28,690 - INFO - Batch 10/20, Loss: 8.1969, LR: 0.000004
2025-08-04 11:43:32,251 - INFO - Epoch 0/3:
2025-08-04 11:43:32,251 - INFO -   Train Loss: 8.2056
2025-08-04 11:43:32,251 - INFO -   Val Loss: 8.1744
2025-08-04 11:43:32,251 - INFO -   Perplexity: 3548.88
2025-08-04 11:43:32,451 - INFO - Saved checkpoint at epoch 0
2025-08-04 11:43:32,802 - INFO - Sample: the array eye develops gleaning non twirled employee published jaunty northern cat depends j kill blame performances themselves — trucks sizes position 13 kinsfolk have per received appleton universit...
2025-08-04 11:43:32,967 - INFO - Saved checkpoint at epoch 0
2025-08-04 11:43:32,967 - INFO - Epoch 0 completed in 7.36s
2025-08-04 11:43:33,329 - INFO - Batch 0/20, Loss: 8.2154, LR: 0.000007
2025-08-04 11:43:36,052 - INFO - Batch 10/20, Loss: 8.1550, LR: 0.000011
2025-08-04 11:43:39,476 - INFO - Epoch 1/3:
2025-08-04 11:43:39,476 - INFO -   Train Loss: 8.1643
2025-08-04 11:43:39,477 - INFO -   Val Loss: 8.1343
2025-08-04 11:43:39,477 - INFO -   Perplexity: 3409.57
2025-08-04 11:43:39,592 - INFO - Saved checkpoint at epoch 1
2025-08-04 11:43:39,904 - INFO - Sample: the largely lots contain alternately opens priced species gratifying builds harvey bread ruskin tough desired alleghanies retired screaming bit scale screech carry shade versa 18 relieve steam oversha...
2025-08-04 11:43:40,093 - INFO - Saved checkpoint at epoch 1
2025-08-04 11:43:40,093 - INFO - Epoch 1 completed in 7.13s
2025-08-04 11:43:40,551 - INFO - Batch 0/20, Loss: 8.1286, LR: 0.000014
2025-08-04 11:43:43,234 - INFO - Batch 10/20, Loss: 8.2006, LR: 0.000018
2025-08-04 11:43:46,621 - INFO - Epoch 2/3:
2025-08-04 11:43:46,621 - INFO -   Train Loss: 8.1105
2025-08-04 11:43:46,621 - INFO -   Val Loss: 8.0650
2025-08-04 11:43:46,621 - INFO -   Perplexity: 3181.10
2025-08-04 11:43:46,855 - INFO - Saved checkpoint at epoch 2
2025-08-04 11:43:47,323 - INFO - Sample: the plumage rapacious writing aware magnificent remarkable expenses juvenile raised fanning fast chi group get gilded 65c hearth _saddle dried corkaline superintendent architecture plumbing bright mea...
2025-08-04 11:43:47,462 - INFO - Saved checkpoint at epoch 2
2025-08-04 11:43:47,462 - INFO - Epoch 2 completed in 7.37s
2025-08-04 11:43:47,462 - INFO - Training completed in 21.85s
2025-08-04 11:43:47,646 - INFO - Saved checkpoint at epoch 3
2025-08-04 11:43:47,646 - INFO - Training finished!
2025-08-04 11:50:53,591 - INFO - Starting transformer training...
2025-08-04 11:50:53,591 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 256,
    "dropout": 0.1,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 4,
    "epochs": 3,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 1,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 512,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "auto",
    "num_workers": 2,
    "pin_memory": false
  },
  "evaluation": {
    "eval_frequency": 1,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-04 11:50:53,648 - INFO - Using device: cuda
2025-08-04 11:50:53,648 - INFO - Train texts: 1, Validation texts: 1
2025-08-04 11:50:53,805 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-04 16:12:50,076 - INFO - Starting transformer training...
2025-08-04 16:12:50,080 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 256,
    "dropout": 0.1,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 4,
    "epochs": 3,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 1,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 512,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "auto",
    "num_workers": 2,
    "pin_memory": true
  },
  "evaluation": {
    "eval_frequency": 1,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-04 16:12:50,081 - INFO - Using device: cpu
2025-08-04 16:12:50,082 - INFO - Train texts: 1, Validation texts: 1
2025-08-04 16:12:50,125 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-04 16:12:50,948 - INFO - Batch 0/20, Loss: 8.3527, LR: 0.000000
2025-08-04 16:12:54,369 - INFO - Batch 10/20, Loss: 8.1987, LR: 0.000004
2025-08-04 16:12:59,169 - INFO - Epoch 0/3:
2025-08-04 16:12:59,169 - INFO -   Train Loss: 8.2039
2025-08-04 16:12:59,169 - INFO -   Val Loss: 8.1745
2025-08-04 16:12:59,170 - INFO -   Perplexity: 3549.35
2025-08-04 16:12:59,400 - INFO - Saved checkpoint at epoch 0
2025-08-04 16:13:00,338 - INFO - Sample: the facing effect confirmation affecting french latter restrictions boy removed past handsome noise compel strain skull build 750 steam our promoting series bitter minstrel but warblers twigs seriousl...
2025-08-04 16:13:01,496 - INFO - Saved checkpoint at epoch 0
2025-08-04 16:13:01,497 - INFO - Epoch 0 completed in 10.92s
2025-08-04 16:13:02,232 - INFO - Batch 0/20, Loss: 8.2040, LR: 0.000007
2025-08-04 16:13:05,733 - INFO - Batch 10/20, Loss: 8.1889, LR: 0.000011
2025-08-04 16:13:10,352 - INFO - Epoch 1/3:
2025-08-04 16:13:10,352 - INFO -   Train Loss: 8.1619
2025-08-04 16:13:10,352 - INFO -   Val Loss: 8.1343
2025-08-04 16:13:10,353 - INFO -   Perplexity: 3409.56
2025-08-04 16:13:10,561 - INFO - Saved checkpoint at epoch 1
2025-08-04 16:13:11,654 - INFO - Sample: the bud 43 ambitious curiously outdated link similar living finer benefactor _mail bookkeeping bluebirds kindly world circles sad guns _for errors soundings sections value comparatively minstrel 20 wo...
2025-08-04 16:13:11,897 - INFO - Saved checkpoint at epoch 1
2025-08-04 16:13:11,897 - INFO - Epoch 1 completed in 10.40s
2025-08-04 16:13:12,602 - INFO - Batch 0/20, Loss: 8.1377, LR: 0.000014
2025-08-04 16:13:15,843 - INFO - Batch 10/20, Loss: 8.0702, LR: 0.000018
2025-08-04 16:13:20,570 - INFO - Epoch 2/3:
2025-08-04 16:13:20,570 - INFO -   Train Loss: 8.1134
2025-08-04 16:13:20,570 - INFO -   Val Loss: 8.0651
2025-08-04 16:13:20,571 - INFO -   Perplexity: 3181.48
2025-08-04 16:13:20,759 - INFO - Saved checkpoint at epoch 2
2025-08-04 16:13:21,644 - INFO - Sample: the unmounted gleaning council truth motionless scrapers works scarcity per taught trouble direction saucily groan seek subsists instruction noiseless needles nest vigorously professor ask males twigs...
2025-08-04 16:13:24,786 - INFO - Saved checkpoint at epoch 2
2025-08-04 16:13:24,787 - INFO - Epoch 2 completed in 12.89s
2025-08-04 16:13:24,788 - INFO - Training completed in 34.22s
2025-08-04 16:13:27,091 - INFO - Saved checkpoint at epoch 3
2025-08-04 16:13:27,092 - INFO - Training finished!
2025-08-04 16:32:54,951 - INFO - Starting transformer training...
2025-08-04 16:32:54,952 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 256,
    "dropout": 0.1,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 4,
    "epochs": 3,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 1,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 512,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "auto",
    "num_workers": 2,
    "pin_memory": true
  },
  "evaluation": {
    "eval_frequency": 1,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-04 16:32:54,953 - INFO - Using device: cpu
2025-08-04 16:32:54,955 - INFO - Train texts: 1, Validation texts: 1
2025-08-04 16:32:55,008 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-04 16:32:56,045 - INFO - Batch 0/20, Loss: 8.3527, LR: 0.000000
2025-08-04 16:32:59,149 - INFO - Batch 10/20, Loss: 8.1987, LR: 0.000004
2025-08-04 16:33:03,590 - INFO - Epoch 0/3:
2025-08-04 16:33:03,590 - INFO -   Train Loss: 8.2039
2025-08-04 16:33:03,591 - INFO -   Val Loss: 8.1745
2025-08-04 16:33:03,591 - INFO -   Perplexity: 3549.35
2025-08-04 16:33:03,792 - INFO - Saved checkpoint at epoch 0
2025-08-04 16:33:04,747 - INFO - Sample: the facing effect confirmation affecting french latter restrictions boy removed past handsome noise compel strain skull build 750 steam our promoting series bitter minstrel but warblers twigs seriousl...
2025-08-04 16:33:04,977 - INFO - Saved checkpoint at epoch 0
2025-08-04 16:33:04,979 - INFO - Epoch 0 completed in 9.45s
2025-08-04 16:33:05,569 - INFO - Batch 0/20, Loss: 8.2040, LR: 0.000007
2025-08-04 16:33:09,170 - INFO - Batch 10/20, Loss: 8.1889, LR: 0.000011
2025-08-04 16:33:13,734 - INFO - Epoch 1/3:
2025-08-04 16:33:13,735 - INFO -   Train Loss: 8.1619
2025-08-04 16:33:13,735 - INFO -   Val Loss: 8.1343
2025-08-04 16:33:13,735 - INFO -   Perplexity: 3409.56
2025-08-04 16:33:13,988 - INFO - Saved checkpoint at epoch 1
2025-08-04 16:33:14,889 - INFO - Sample: the bud 43 ambitious curiously outdated link similar living finer benefactor _mail bookkeeping bluebirds kindly world circles sad guns _for errors soundings sections value comparatively minstrel 20 wo...
2025-08-04 16:33:15,104 - INFO - Saved checkpoint at epoch 1
2025-08-04 16:33:15,104 - INFO - Epoch 1 completed in 10.12s
2025-08-04 16:33:15,723 - INFO - Batch 0/20, Loss: 8.1377, LR: 0.000014
2025-08-04 16:33:19,121 - INFO - Batch 10/20, Loss: 8.0702, LR: 0.000018
2025-08-04 16:33:23,935 - INFO - Epoch 2/3:
2025-08-04 16:33:23,935 - INFO -   Train Loss: 8.1134
2025-08-04 16:33:23,936 - INFO -   Val Loss: 8.0651
2025-08-04 16:33:23,936 - INFO -   Perplexity: 3181.48
2025-08-04 16:33:25,108 - INFO - Saved checkpoint at epoch 2
2025-08-04 16:33:26,023 - INFO - Sample: the unmounted gleaning council truth motionless scrapers works scarcity per taught trouble direction saucily groan seek subsists instruction noiseless needles nest vigorously professor ask males twigs...
2025-08-04 16:33:26,247 - INFO - Saved checkpoint at epoch 2
2025-08-04 16:33:26,247 - INFO - Epoch 2 completed in 11.14s
2025-08-04 16:33:26,248 - INFO - Training completed in 30.72s
2025-08-04 16:33:26,427 - INFO - Saved checkpoint at epoch 3
2025-08-04 16:33:26,428 - INFO - Training finished!
2025-08-04 16:45:24,186 - INFO - Starting transformer training...
2025-08-04 16:45:24,190 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 256,
    "dropout": 0.1,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 4,
    "epochs": 3,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 1,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 512,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "auto",
    "num_workers": 2,
    "pin_memory": true
  },
  "evaluation": {
    "eval_frequency": 1,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-04 16:45:24,246 - INFO - Using device: cuda
2025-08-04 16:45:24,247 - INFO - Train texts: 1, Validation texts: 1
2025-08-04 16:45:24,394 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-04 16:50:34,458 - INFO - Starting transformer training...
2025-08-04 16:50:34,460 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 256,
    "dropout": 0.1,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 4,
    "epochs": 3,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 1,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 512,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "auto",
    "num_workers": 2,
    "pin_memory": true
  },
  "evaluation": {
    "eval_frequency": 1,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-04 16:50:34,517 - INFO - Using device: cuda
2025-08-04 16:50:34,518 - INFO - Train texts: 1, Validation texts: 1
2025-08-04 16:50:34,667 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-04 16:51:03,440 - INFO - Starting transformer training...
2025-08-04 16:51:03,440 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 256,
    "dropout": 0.1,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 4,
    "epochs": 3,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 1,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 512,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "cpu",
    "num_workers": 2,
    "pin_memory": true
  },
  "evaluation": {
    "eval_frequency": 1,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-04 16:51:03,441 - INFO - Using device: cpu
2025-08-04 16:51:03,441 - INFO - Train texts: 1, Validation texts: 1
2025-08-04 16:51:03,479 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-04 16:51:04,893 - INFO - Batch 0/20, Loss: 8.3527, LR: 0.000000
2025-08-04 16:51:11,592 - INFO - Batch 10/20, Loss: 8.1987, LR: 0.000004
2025-08-04 16:51:20,722 - INFO - Epoch 0/3:
2025-08-04 16:51:20,722 - INFO -   Train Loss: 8.2039
2025-08-04 16:51:20,723 - INFO -   Val Loss: 8.1745
2025-08-04 16:51:20,723 - INFO -   Perplexity: 3549.35
2025-08-04 16:51:20,903 - INFO - Saved checkpoint at epoch 0
2025-08-04 16:51:21,743 - INFO - Sample: the facing effect confirmation affecting french latter restrictions boy removed past handsome noise compel strain skull build 750 steam our promoting series bitter minstrel but warblers twigs seriousl...
2025-08-04 16:51:21,921 - INFO - Saved checkpoint at epoch 0
2025-08-04 16:51:21,921 - INFO - Epoch 0 completed in 18.02s
2025-08-04 16:51:22,701 - INFO - Batch 0/20, Loss: 8.2040, LR: 0.000007
2025-08-04 16:51:29,342 - INFO - Batch 10/20, Loss: 8.1889, LR: 0.000011
2025-08-04 16:51:38,479 - INFO - Epoch 1/3:
2025-08-04 16:51:38,479 - INFO -   Train Loss: 8.1619
2025-08-04 16:51:38,479 - INFO -   Val Loss: 8.1343
2025-08-04 16:51:38,479 - INFO -   Perplexity: 3409.56
2025-08-04 16:51:40,429 - INFO - Saved checkpoint at epoch 1
2025-08-04 16:51:41,237 - INFO - Sample: the bud 43 ambitious curiously outdated link similar living finer benefactor _mail bookkeeping bluebirds kindly world circles sad guns _for errors soundings sections value comparatively minstrel 20 wo...
2025-08-04 16:51:41,398 - INFO - Saved checkpoint at epoch 1
2025-08-04 16:51:41,398 - INFO - Epoch 1 completed in 19.48s
2025-08-04 16:51:42,203 - INFO - Batch 0/20, Loss: 8.1377, LR: 0.000014
2025-08-04 16:51:48,879 - INFO - Batch 10/20, Loss: 8.0702, LR: 0.000018
2025-08-04 16:51:57,896 - INFO - Epoch 2/3:
2025-08-04 16:51:57,896 - INFO -   Train Loss: 8.1134
2025-08-04 16:51:57,896 - INFO -   Val Loss: 8.0651
2025-08-04 16:51:57,896 - INFO -   Perplexity: 3181.48
2025-08-04 16:51:58,101 - INFO - Saved checkpoint at epoch 2
2025-08-04 16:51:58,866 - INFO - Sample: the unmounted gleaning council truth motionless scrapers works scarcity per taught trouble direction saucily groan seek subsists instruction noiseless needles nest vigorously professor ask males twigs...
2025-08-04 16:51:59,066 - INFO - Saved checkpoint at epoch 2
2025-08-04 16:51:59,066 - INFO - Epoch 2 completed in 17.67s
2025-08-04 16:51:59,066 - INFO - Training completed in 55.16s
2025-08-04 16:51:59,301 - INFO - Saved checkpoint at epoch 3
2025-08-04 16:51:59,311 - INFO - Training finished!
2025-08-04 17:00:36,537 - INFO - Starting transformer training...
2025-08-04 17:00:36,537 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 256,
    "dropout": 0.1,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 4,
    "epochs": 3,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 1,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 512,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "cpu",
    "num_workers": 2,
    "pin_memory": true
  },
  "evaluation": {
    "eval_frequency": 1,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-04 17:00:36,537 - INFO - Using device: cpu
2025-08-04 17:00:36,538 - INFO - Train texts: 1, Validation texts: 1
2025-08-04 17:00:36,585 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-04 17:00:37,932 - INFO - Batch 0/20, Loss: 8.3527, LR: 0.000000
2025-08-04 17:00:44,400 - INFO - Batch 10/20, Loss: 8.1987, LR: 0.000004
2025-08-04 17:00:53,519 - INFO - Epoch 0/3:
2025-08-04 17:00:53,520 - INFO -   Train Loss: 8.2039
2025-08-04 17:00:53,520 - INFO -   Val Loss: 8.1745
2025-08-04 17:00:53,520 - INFO -   Perplexity: 3549.35
2025-08-04 17:00:53,700 - INFO - Saved checkpoint at epoch 0
2025-08-04 17:00:54,496 - INFO - Sample: the facing effect confirmation affecting french latter restrictions boy removed past handsome noise compel strain skull build 750 steam our promoting series bitter minstrel but warblers twigs seriousl...
2025-08-04 17:00:54,696 - INFO - Saved checkpoint at epoch 0
2025-08-04 17:00:54,696 - INFO - Epoch 0 completed in 17.71s
2025-08-04 17:00:55,497 - INFO - Batch 0/20, Loss: 8.2040, LR: 0.000007
2025-08-04 17:01:02,257 - INFO - Batch 10/20, Loss: 8.1889, LR: 0.000011
2025-08-04 17:01:11,504 - INFO - Epoch 1/3:
2025-08-04 17:01:11,504 - INFO -   Train Loss: 8.1619
2025-08-04 17:01:11,505 - INFO -   Val Loss: 8.1343
2025-08-04 17:01:11,505 - INFO -   Perplexity: 3409.56
2025-08-04 17:01:11,686 - INFO - Saved checkpoint at epoch 1
2025-08-04 17:01:12,516 - INFO - Sample: the bud 43 ambitious curiously outdated link similar living finer benefactor _mail bookkeeping bluebirds kindly world circles sad guns _for errors soundings sections value comparatively minstrel 20 wo...
2025-08-04 17:01:12,725 - INFO - Saved checkpoint at epoch 1
2025-08-04 17:01:12,725 - INFO - Epoch 1 completed in 18.03s
2025-08-04 17:01:13,572 - INFO - Batch 0/20, Loss: 8.1377, LR: 0.000014
2025-08-04 17:01:20,478 - INFO - Batch 10/20, Loss: 8.0702, LR: 0.000018
2025-08-04 17:01:29,974 - INFO - Epoch 2/3:
2025-08-04 17:01:29,974 - INFO -   Train Loss: 8.1134
2025-08-04 17:01:29,974 - INFO -   Val Loss: 8.0651
2025-08-04 17:01:29,974 - INFO -   Perplexity: 3181.48
2025-08-04 17:01:30,147 - INFO - Saved checkpoint at epoch 2
2025-08-04 17:01:31,004 - INFO - Sample: the unmounted gleaning council truth motionless scrapers works scarcity per taught trouble direction saucily groan seek subsists instruction noiseless needles nest vigorously professor ask males twigs...
2025-08-04 17:01:32,180 - INFO - Saved checkpoint at epoch 2
2025-08-04 17:01:32,180 - INFO - Epoch 2 completed in 19.46s
2025-08-04 17:01:32,181 - INFO - Training completed in 55.20s
2025-08-04 17:01:33,819 - INFO - Saved checkpoint at epoch 3
2025-08-04 17:01:33,819 - INFO - Training finished!
2025-08-11 14:44:36,309 - INFO - Starting transformer training...
2025-08-11 14:44:36,309 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 256,
    "dropout": 0.1,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 4,
    "epochs": 3,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 1,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 512,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "cpu",
    "num_workers": 2,
    "pin_memory": true
  },
  "evaluation": {
    "eval_frequency": 1,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-11 14:44:36,309 - INFO - Using device: cpu
2025-08-11 14:44:36,309 - INFO - Train texts: 1, Validation texts: 1
2025-08-11 14:44:36,383 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-11 14:44:39,335 - INFO - Batch 0/20, Loss: 8.3716, LR: 0.000000
2025-08-11 14:45:31,921 - INFO - Starting transformer training...
2025-08-11 14:45:31,921 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 256,
    "dropout": 0.1,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 4,
    "epochs": 3,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 1,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 512,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "cpu",
    "num_workers": 2,
    "pin_memory": true
  },
  "evaluation": {
    "eval_frequency": 1,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-11 14:45:31,921 - INFO - Using device: cpu
2025-08-11 14:45:31,922 - INFO - Train texts: 1, Validation texts: 1
2025-08-11 14:45:31,958 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-11 14:45:33,638 - INFO - Batch 0/20, Loss: 8.3716, LR: 0.000000
2025-08-11 14:45:40,768 - INFO - Batch 10/20, Loss: 8.1969, LR: 0.000004
2025-08-11 14:45:50,471 - INFO - Epoch 0/3:
2025-08-11 14:45:50,471 - INFO -   Train Loss: 8.2056
2025-08-11 14:45:50,471 - INFO -   Val Loss: 8.1744
2025-08-11 14:45:50,471 - INFO -   Perplexity: 3548.88
2025-08-11 14:45:50,695 - INFO - Saved checkpoint at epoch 0
2025-08-11 14:45:51,510 - INFO - Sample: the array eye develops gleaning non twirled employee published jaunty northern cat depends j kill blame performances themselves — trucks sizes position 13 kinsfolk have per received appleton universit...
2025-08-11 14:45:51,703 - INFO - Saved checkpoint at epoch 0
2025-08-11 14:45:51,704 - INFO - Epoch 0 completed in 19.10s
2025-08-11 14:45:52,541 - INFO - Batch 0/20, Loss: 8.2154, LR: 0.000007
2025-08-11 14:45:59,688 - INFO - Batch 10/20, Loss: 8.1550, LR: 0.000011
2025-08-11 14:46:09,160 - INFO - Epoch 1/3:
2025-08-11 14:46:09,161 - INFO -   Train Loss: 8.1643
2025-08-11 14:46:09,161 - INFO -   Val Loss: 8.1343
2025-08-11 14:46:09,161 - INFO -   Perplexity: 3409.57
2025-08-11 14:46:09,391 - INFO - Saved checkpoint at epoch 1
2025-08-11 14:46:10,231 - INFO - Sample: the largely lots contain alternately opens priced species gratifying builds harvey bread ruskin tough desired alleghanies retired screaming bit scale screech carry shade versa 18 relieve steam oversha...
2025-08-11 14:46:10,409 - INFO - Saved checkpoint at epoch 1
2025-08-11 14:46:10,409 - INFO - Epoch 1 completed in 18.71s
2025-08-11 14:46:11,219 - INFO - Batch 0/20, Loss: 8.1286, LR: 0.000014
2025-08-11 14:46:18,299 - INFO - Batch 10/20, Loss: 8.2006, LR: 0.000018
2025-08-11 14:46:27,631 - INFO - Epoch 2/3:
2025-08-11 14:46:27,631 - INFO -   Train Loss: 8.1105
2025-08-11 14:46:27,631 - INFO -   Val Loss: 8.0650
2025-08-11 14:46:27,631 - INFO -   Perplexity: 3181.10
2025-08-11 14:46:27,820 - INFO - Saved checkpoint at epoch 2
2025-08-11 14:46:28,626 - INFO - Sample: the plumage rapacious writing aware magnificent remarkable expenses juvenile raised fanning fast chi group get gilded 65c hearth _saddle dried corkaline superintendent architecture plumbing bright mea...
2025-08-11 14:46:28,803 - INFO - Saved checkpoint at epoch 2
2025-08-11 14:46:28,803 - INFO - Epoch 2 completed in 18.39s
2025-08-11 14:46:28,803 - INFO - Training completed in 56.19s
2025-08-11 14:46:29,060 - INFO - Saved checkpoint at epoch 3
2025-08-11 14:46:29,061 - INFO - Training finished!
2025-08-14 12:13:38,777 - INFO - Starting transformer training...
2025-08-14 12:13:38,778 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 1024,
    "dropout": 0.1,
    "src_vocab_size": 5000,
    "tgt_vocab_size": 5000
  },
  "training": {
    "batch_size": 4,
    "epochs": 5,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 100,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 2,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 128,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "cpu",
    "num_workers": 2,
    "pin_memory": false
  },
  "evaluation": {
    "eval_frequency": 5,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-14 12:13:38,778 - INFO - Using device: cpu
2025-08-14 12:13:38,778 - INFO - Train texts: 1, Validation texts: 1
2025-08-14 12:13:38,814 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-14 12:13:39,519 - INFO - Batch 0/80, Loss: 8.3858, LR: 0.000088
2025-08-14 12:13:39,940 - INFO - Batch 10/80, Loss: 7.4111, LR: 0.000972
2025-08-14 12:13:40,336 - INFO - Batch 20/80, Loss: 6.5957, LR: 0.001856
2025-08-14 12:13:40,727 - INFO - Batch 30/80, Loss: 6.4169, LR: 0.002740
2025-08-14 12:13:41,125 - INFO - Batch 40/80, Loss: 6.5444, LR: 0.003624
2025-08-14 12:13:41,516 - INFO - Batch 50/80, Loss: 6.2570, LR: 0.004508
2025-08-14 12:13:41,904 - INFO - Batch 60/80, Loss: 6.1875, LR: 0.005392
2025-08-14 12:13:42,284 - INFO - Batch 70/80, Loss: 5.5108, LR: 0.006276
2025-08-14 12:13:43,340 - INFO - Epoch 0/5:
2025-08-14 12:13:43,340 - INFO -   Train Loss: 6.6057
2025-08-14 12:13:43,340 - INFO -   Val Loss: 5.6470
2025-08-14 12:13:43,340 - INFO -   Perplexity: 283.44
2025-08-14 12:13:43,392 - INFO - Saved checkpoint at epoch 0
2025-08-14 12:13:43,696 - INFO - Sample: the bottom ™ ™ a bird , # the edges . the trademark . w delicate or ] the our happy and __________ is a compliance . it is cycle bird to ' the edges scissor . it set hop . be paragraph , and however i...
2025-08-14 12:13:43,751 - INFO - Saved checkpoint at epoch 0
2025-08-14 12:13:43,752 - INFO - Epoch 0 completed in 4.44s
2025-08-14 12:13:43,830 - INFO - Batch 0/80, Loss: 5.8795, LR: 0.007159
2025-08-14 12:13:44,231 - INFO - Batch 10/80, Loss: 5.6708, LR: 0.008043
2025-08-14 12:13:44,622 - INFO - Batch 20/80, Loss: 5.1999, LR: 0.008795
2025-08-14 12:13:45,030 - INFO - Batch 30/80, Loss: 5.6811, LR: 0.008389
2025-08-14 12:13:45,431 - INFO - Batch 40/80, Loss: 5.7041, LR: 0.008035
2025-08-14 12:13:45,831 - INFO - Batch 50/80, Loss: 5.2061, LR: 0.007723
2025-08-14 12:13:46,238 - INFO - Batch 60/80, Loss: 5.0616, LR: 0.007444
2025-08-14 12:13:46,615 - INFO - Batch 70/80, Loss: 5.0497, LR: 0.007193
2025-08-14 12:13:46,954 - INFO - Epoch 1 completed in 3.20s
2025-08-14 12:13:47,024 - INFO - Batch 0/80, Loss: 4.5802, LR: 0.006966
2025-08-14 12:13:47,392 - INFO - Batch 10/80, Loss: 4.8749, LR: 0.006759
2025-08-14 12:13:47,759 - INFO - Batch 20/80, Loss: 4.7258, LR: 0.006570
2025-08-14 12:13:48,166 - INFO - Batch 30/80, Loss: 4.9892, LR: 0.006396
2025-08-14 12:13:48,557 - INFO - Batch 40/80, Loss: 4.6280, LR: 0.006234
2025-08-14 12:13:48,919 - INFO - Batch 50/80, Loss: 4.2301, LR: 0.006085
2025-08-14 12:13:49,286 - INFO - Batch 60/80, Loss: 4.7276, LR: 0.005946
2025-08-14 12:13:49,664 - INFO - Batch 70/80, Loss: 4.7332, LR: 0.005816
2025-08-14 12:13:50,047 - INFO - Saved checkpoint at epoch 2
2025-08-14 12:13:50,047 - INFO - Epoch 2 completed in 3.09s
2025-08-14 12:13:50,120 - INFO - Batch 0/80, Loss: 4.2307, LR: 0.005694
2025-08-14 12:13:50,490 - INFO - Batch 10/80, Loss: 4.0808, LR: 0.005579
2025-08-14 12:13:50,862 - INFO - Batch 20/80, Loss: 4.0496, LR: 0.005471
2025-08-14 12:13:51,235 - INFO - Batch 30/80, Loss: 3.5744, LR: 0.005369
2025-08-14 12:13:51,605 - INFO - Batch 40/80, Loss: 4.2645, LR: 0.005273
2025-08-14 12:13:51,976 - INFO - Batch 50/80, Loss: 3.6297, LR: 0.005181
2025-08-14 12:13:52,350 - INFO - Batch 60/80, Loss: 3.8918, LR: 0.005095
2025-08-14 12:13:52,716 - INFO - Batch 70/80, Loss: 3.9243, LR: 0.005012
2025-08-14 12:13:53,050 - INFO - Epoch 3 completed in 3.00s
2025-08-14 12:13:53,129 - INFO - Batch 0/80, Loss: 3.5463, LR: 0.004933
2025-08-14 12:13:53,502 - INFO - Batch 10/80, Loss: 3.3336, LR: 0.004858
2025-08-14 12:13:53,882 - INFO - Batch 20/80, Loss: 3.2440, LR: 0.004786
2025-08-14 12:13:54,291 - INFO - Batch 30/80, Loss: 3.7609, LR: 0.004718
2025-08-14 12:13:54,654 - INFO - Batch 40/80, Loss: 3.5478, LR: 0.004652
2025-08-14 12:13:55,017 - INFO - Batch 50/80, Loss: 3.8796, LR: 0.004589
2025-08-14 12:13:55,456 - INFO - Batch 60/80, Loss: 3.2104, LR: 0.004528
2025-08-14 12:13:55,818 - INFO - Batch 70/80, Loss: 3.6749, LR: 0.004470
2025-08-14 12:13:56,234 - INFO - Saved checkpoint at epoch 4
2025-08-14 12:13:56,235 - INFO - Epoch 4 completed in 3.18s
2025-08-14 12:13:56,235 - INFO - Training completed in 16.93s
2025-08-14 12:13:56,389 - INFO - Saved checkpoint at epoch 5
2025-08-14 12:13:56,389 - INFO - Training finished!
2025-08-14 12:14:22,264 - INFO - Starting transformer training...
2025-08-14 12:14:22,264 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 1024,
    "dropout": 0.1,
    "src_vocab_size": 5000,
    "tgt_vocab_size": 5000
  },
  "training": {
    "batch_size": 4,
    "epochs": 5,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 100,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 2,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 128,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "cpu",
    "num_workers": 2,
    "pin_memory": false
  },
  "evaluation": {
    "eval_frequency": 5,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-14 12:14:22,264 - INFO - Using device: cpu
2025-08-14 12:14:22,265 - INFO - Train texts: 1, Validation texts: 1
2025-08-14 12:14:22,300 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-14 12:14:22,929 - INFO - Batch 0/80, Loss: 8.3858, LR: 0.000088
2025-08-14 12:14:23,338 - INFO - Batch 10/80, Loss: 7.4111, LR: 0.000972
2025-08-14 12:14:23,737 - INFO - Batch 20/80, Loss: 6.5957, LR: 0.001856
2025-08-14 12:14:24,150 - INFO - Batch 30/80, Loss: 6.4169, LR: 0.002740
2025-08-14 12:14:24,577 - INFO - Batch 40/80, Loss: 6.5444, LR: 0.003624
2025-08-14 12:14:24,971 - INFO - Batch 50/80, Loss: 6.2570, LR: 0.004508
2025-08-14 12:14:25,367 - INFO - Batch 60/80, Loss: 6.1875, LR: 0.005392
2025-08-14 12:14:25,756 - INFO - Batch 70/80, Loss: 5.5108, LR: 0.006276
2025-08-14 12:14:26,735 - INFO - Epoch 0/5:
2025-08-14 12:14:26,736 - INFO -   Train Loss: 6.6057
2025-08-14 12:14:26,736 - INFO -   Val Loss: 5.6470
2025-08-14 12:14:26,736 - INFO -   Perplexity: 283.44
2025-08-14 12:14:26,791 - INFO - Saved checkpoint at epoch 0
2025-08-14 12:14:27,057 - INFO - Sample: the bottom ™ ™ a bird , # the edges . the trademark . w delicate or ] the our happy and __________ is a compliance . it is cycle bird to ' the edges scissor . it set hop . be paragraph , and however i...
2025-08-14 12:14:27,108 - INFO - Saved checkpoint at epoch 0
2025-08-14 12:14:27,108 - INFO - Epoch 0 completed in 4.33s
2025-08-14 12:14:27,181 - INFO - Batch 0/80, Loss: 5.8795, LR: 0.007159
2025-08-14 12:14:27,592 - INFO - Batch 10/80, Loss: 5.6708, LR: 0.008043
2025-08-14 12:14:28,009 - INFO - Batch 20/80, Loss: 5.1999, LR: 0.008795
2025-08-14 12:14:28,401 - INFO - Batch 30/80, Loss: 5.6811, LR: 0.008389
2025-08-14 12:14:28,849 - INFO - Batch 40/80, Loss: 5.7041, LR: 0.008035
2025-08-14 12:14:29,309 - INFO - Batch 50/80, Loss: 5.2061, LR: 0.007723
2025-08-14 12:14:29,692 - INFO - Batch 60/80, Loss: 5.0616, LR: 0.007444
2025-08-14 12:14:30,072 - INFO - Batch 70/80, Loss: 5.0497, LR: 0.007193
2025-08-14 12:14:30,424 - INFO - Epoch 1 completed in 3.32s
2025-08-14 12:14:30,501 - INFO - Batch 0/80, Loss: 4.5802, LR: 0.006966
2025-08-14 12:14:30,885 - INFO - Batch 10/80, Loss: 4.8749, LR: 0.006759
2025-08-14 12:14:31,257 - INFO - Batch 20/80, Loss: 4.7258, LR: 0.006570
2025-08-14 12:14:31,640 - INFO - Batch 30/80, Loss: 4.9892, LR: 0.006396
2025-08-14 12:14:32,018 - INFO - Batch 40/80, Loss: 4.6280, LR: 0.006234
2025-08-14 12:14:32,397 - INFO - Batch 50/80, Loss: 4.2301, LR: 0.006085
2025-08-14 12:14:32,786 - INFO - Batch 60/80, Loss: 4.7276, LR: 0.005946
2025-08-14 12:14:33,147 - INFO - Batch 70/80, Loss: 4.7332, LR: 0.005816
2025-08-14 12:14:33,531 - INFO - Saved checkpoint at epoch 2
2025-08-14 12:14:33,532 - INFO - Epoch 2 completed in 3.11s
2025-08-14 12:14:33,605 - INFO - Batch 0/80, Loss: 4.2307, LR: 0.005694
2025-08-14 12:14:33,975 - INFO - Batch 10/80, Loss: 4.0808, LR: 0.005579
2025-08-14 12:14:34,463 - INFO - Batch 20/80, Loss: 4.0496, LR: 0.005471
2025-08-14 12:14:34,929 - INFO - Batch 30/80, Loss: 3.5744, LR: 0.005369
2025-08-14 12:14:35,305 - INFO - Batch 40/80, Loss: 4.2645, LR: 0.005273
2025-08-14 12:14:35,697 - INFO - Batch 50/80, Loss: 3.6297, LR: 0.005181
2025-08-14 12:14:36,079 - INFO - Batch 60/80, Loss: 3.8918, LR: 0.005095
2025-08-14 12:14:36,463 - INFO - Batch 70/80, Loss: 3.9243, LR: 0.005012
2025-08-14 12:14:36,801 - INFO - Epoch 3 completed in 3.27s
2025-08-14 12:14:36,872 - INFO - Batch 0/80, Loss: 3.5463, LR: 0.004933
2025-08-14 12:14:37,246 - INFO - Batch 10/80, Loss: 3.3336, LR: 0.004858
2025-08-14 12:14:37,655 - INFO - Batch 20/80, Loss: 3.2440, LR: 0.004786
2025-08-14 12:14:38,025 - INFO - Batch 30/80, Loss: 3.7609, LR: 0.004718
2025-08-14 12:14:38,390 - INFO - Batch 40/80, Loss: 3.5478, LR: 0.004652
2025-08-14 12:14:38,755 - INFO - Batch 50/80, Loss: 3.8796, LR: 0.004589
2025-08-14 12:14:39,125 - INFO - Batch 60/80, Loss: 3.2104, LR: 0.004528
2025-08-14 12:14:39,488 - INFO - Batch 70/80, Loss: 3.6749, LR: 0.004470
2025-08-14 12:14:39,873 - INFO - Saved checkpoint at epoch 4
2025-08-14 12:14:39,873 - INFO - Epoch 4 completed in 3.07s
2025-08-14 12:14:39,873 - INFO - Training completed in 17.10s
2025-08-14 12:14:40,000 - INFO - Saved checkpoint at epoch 5
2025-08-14 12:14:40,000 - INFO - Training finished!
2025-08-14 12:14:45,255 - INFO - Starting transformer training...
2025-08-14 12:14:45,255 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 1024,
    "dropout": 0.1,
    "src_vocab_size": 5000,
    "tgt_vocab_size": 5000
  },
  "training": {
    "batch_size": 4,
    "epochs": 5,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 100,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 2,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 128,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "auto",
    "num_workers": 2,
    "pin_memory": false
  },
  "evaluation": {
    "eval_frequency": 5,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-14 12:14:45,319 - INFO - Using device: cuda
2025-08-14 12:14:45,320 - INFO - Train texts: 1, Validation texts: 1
2025-08-14 12:14:45,465 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-14 12:14:46,120 - INFO - Batch 0/80, Loss: 8.3896, LR: 0.000088
2025-08-14 12:14:46,175 - INFO - Batch 10/80, Loss: 7.4193, LR: 0.000972
2025-08-14 12:14:46,229 - INFO - Batch 20/80, Loss: 6.5829, LR: 0.001856
2025-08-14 12:14:46,281 - INFO - Batch 30/80, Loss: 6.3937, LR: 0.002740
2025-08-14 12:14:46,333 - INFO - Batch 40/80, Loss: 6.5426, LR: 0.003624
2025-08-14 12:14:46,385 - INFO - Batch 50/80, Loss: 6.2437, LR: 0.004508
2025-08-14 12:14:46,439 - INFO - Batch 60/80, Loss: 6.1285, LR: 0.005392
2025-08-14 12:14:46,491 - INFO - Batch 70/80, Loss: 5.5057, LR: 0.006276
2025-08-14 12:14:46,725 - INFO - Epoch 0/5:
2025-08-14 12:14:46,725 - INFO -   Train Loss: 6.5879
2025-08-14 12:14:46,725 - INFO -   Val Loss: 5.5033
2025-08-14 12:14:46,725 - INFO -   Perplexity: 245.50
2025-08-14 12:14:46,791 - INFO - Saved checkpoint at epoch 0
2025-08-14 12:14:47,149 - INFO - Sample: the foundation , for own , and leafy , and acquaintances of the singers you whenever bold , and in april . it is 97 gutenberg ' s of the ground ™ . chi , and times to with frequently , , - - lights , ...
2025-08-14 12:14:47,216 - INFO - Saved checkpoint at epoch 0
2025-08-14 12:14:47,216 - INFO - Epoch 0 completed in 1.28s
2025-08-14 12:14:47,244 - INFO - Batch 0/80, Loss: 5.3236, LR: 0.007159
2025-08-14 12:14:47,298 - INFO - Batch 10/80, Loss: 5.6803, LR: 0.008043
2025-08-14 12:14:47,352 - INFO - Batch 20/80, Loss: 5.8663, LR: 0.008795
2025-08-14 12:14:47,405 - INFO - Batch 30/80, Loss: 5.3661, LR: 0.008389
2025-08-14 12:14:47,459 - INFO - Batch 40/80, Loss: 5.3982, LR: 0.008035
2025-08-14 12:14:47,513 - INFO - Batch 50/80, Loss: 5.3350, LR: 0.007723
2025-08-14 12:14:47,565 - INFO - Batch 60/80, Loss: 5.2112, LR: 0.007444
2025-08-14 12:14:47,621 - INFO - Batch 70/80, Loss: 5.2770, LR: 0.007193
2025-08-14 12:14:47,680 - INFO - Epoch 1 completed in 0.46s
2025-08-14 12:14:47,706 - INFO - Batch 0/80, Loss: 4.7472, LR: 0.006966
2025-08-14 12:14:47,762 - INFO - Batch 10/80, Loss: 4.9405, LR: 0.006759
2025-08-14 12:14:47,817 - INFO - Batch 20/80, Loss: 4.5227, LR: 0.006570
2025-08-14 12:14:47,873 - INFO - Batch 30/80, Loss: 4.6101, LR: 0.006396
2025-08-14 12:14:47,927 - INFO - Batch 40/80, Loss: 4.0704, LR: 0.006234
2025-08-14 12:14:47,982 - INFO - Batch 50/80, Loss: 3.6850, LR: 0.006085
2025-08-14 12:14:48,037 - INFO - Batch 60/80, Loss: 4.3772, LR: 0.005946
2025-08-14 12:14:48,093 - INFO - Batch 70/80, Loss: 4.6823, LR: 0.005816
2025-08-14 12:14:48,217 - INFO - Saved checkpoint at epoch 2
2025-08-14 12:14:48,217 - INFO - Epoch 2 completed in 0.54s
2025-08-14 12:14:48,244 - INFO - Batch 0/80, Loss: 3.3755, LR: 0.005694
2025-08-14 12:14:48,299 - INFO - Batch 10/80, Loss: 3.6275, LR: 0.005579
2025-08-14 12:14:48,355 - INFO - Batch 20/80, Loss: 3.9194, LR: 0.005471
2025-08-14 12:14:48,410 - INFO - Batch 30/80, Loss: 3.9001, LR: 0.005369
2025-08-14 12:14:48,466 - INFO - Batch 40/80, Loss: 3.4978, LR: 0.005273
2025-08-14 12:14:48,520 - INFO - Batch 50/80, Loss: 3.8672, LR: 0.005181
2025-08-14 12:14:48,576 - INFO - Batch 60/80, Loss: 3.8067, LR: 0.005095
2025-08-14 12:14:48,630 - INFO - Batch 70/80, Loss: 4.0262, LR: 0.005012
2025-08-14 12:14:48,688 - INFO - Epoch 3 completed in 0.47s
2025-08-14 12:14:48,714 - INFO - Batch 0/80, Loss: 2.9678, LR: 0.004933
2025-08-14 12:14:48,772 - INFO - Batch 10/80, Loss: 3.6080, LR: 0.004858
2025-08-14 12:14:48,826 - INFO - Batch 20/80, Loss: 3.4001, LR: 0.004786
2025-08-14 12:14:48,880 - INFO - Batch 30/80, Loss: 3.6736, LR: 0.004718
2025-08-14 12:14:48,935 - INFO - Batch 40/80, Loss: 3.7693, LR: 0.004652
2025-08-14 12:14:48,990 - INFO - Batch 50/80, Loss: 3.9377, LR: 0.004589
2025-08-14 12:14:49,044 - INFO - Batch 60/80, Loss: 3.1738, LR: 0.004528
2025-08-14 12:14:49,100 - INFO - Batch 70/80, Loss: 2.7418, LR: 0.004470
2025-08-14 12:14:49,224 - INFO - Saved checkpoint at epoch 4
2025-08-14 12:14:49,224 - INFO - Epoch 4 completed in 0.54s
2025-08-14 12:14:49,224 - INFO - Training completed in 3.29s
2025-08-14 12:14:49,392 - INFO - Saved checkpoint at epoch 5
2025-08-14 12:14:49,392 - INFO - Training finished!
2025-08-14 12:20:06,304 - INFO - Starting transformer training...
2025-08-14 12:20:06,304 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 1024,
    "dropout": 0.1,
    "src_vocab_size": 5000,
    "tgt_vocab_size": 5000
  },
  "training": {
    "batch_size": 4,
    "epochs": 5,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 100,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 2,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 128,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "auto",
    "num_workers": 2,
    "pin_memory": false
  },
  "evaluation": {
    "eval_frequency": 5,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-14 12:20:06,433 - INFO - Using device: cuda
2025-08-14 12:20:06,434 - INFO - Train texts: 1, Validation texts: 1
2025-08-14 12:20:06,585 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-14 12:20:07,245 - INFO - Batch 0/80, Loss: 8.3896, LR: 0.000088
2025-08-14 12:20:07,304 - INFO - Batch 10/80, Loss: 7.4193, LR: 0.000972
2025-08-14 12:20:07,362 - INFO - Batch 20/80, Loss: 6.5829, LR: 0.001856
2025-08-14 12:20:07,419 - INFO - Batch 30/80, Loss: 6.3937, LR: 0.002740
2025-08-14 12:20:07,475 - INFO - Batch 40/80, Loss: 6.5426, LR: 0.003624
2025-08-14 12:20:07,530 - INFO - Batch 50/80, Loss: 6.2437, LR: 0.004508
2025-08-14 12:20:07,584 - INFO - Batch 60/80, Loss: 6.1285, LR: 0.005392
2025-08-14 12:20:07,639 - INFO - Batch 70/80, Loss: 5.5057, LR: 0.006276
2025-08-14 12:20:07,892 - INFO - Epoch 0/5:
2025-08-14 12:20:07,892 - INFO -   Train Loss: 6.5879
2025-08-14 12:20:07,892 - INFO -   Val Loss: 5.5033
2025-08-14 12:20:07,892 - INFO -   Perplexity: 245.50
2025-08-14 12:20:07,959 - INFO - Saved checkpoint at epoch 0
2025-08-14 12:20:08,336 - INFO - Sample: the foundation , for own , and leafy , and acquaintances of the singers you whenever bold , and in april . it is 97 gutenberg ' s of the ground ™ . chi , and times to with frequently , , - - lights , ...
2025-08-14 12:20:08,425 - INFO - Saved checkpoint at epoch 0
2025-08-14 12:20:08,426 - INFO - Epoch 0 completed in 1.37s
2025-08-14 12:20:08,452 - INFO - Batch 0/80, Loss: 5.3236, LR: 0.007159
2025-08-14 12:20:08,506 - INFO - Batch 10/80, Loss: 5.6803, LR: 0.008043
2025-08-14 12:20:08,560 - INFO - Batch 20/80, Loss: 5.8663, LR: 0.008795
2025-08-14 12:20:08,614 - INFO - Batch 30/80, Loss: 5.3661, LR: 0.008389
2025-08-14 12:20:08,667 - INFO - Batch 40/80, Loss: 5.3982, LR: 0.008035
2025-08-14 12:20:08,720 - INFO - Batch 50/80, Loss: 5.3350, LR: 0.007723
2025-08-14 12:20:08,774 - INFO - Batch 60/80, Loss: 5.2112, LR: 0.007444
2025-08-14 12:20:08,829 - INFO - Batch 70/80, Loss: 5.2770, LR: 0.007193
2025-08-14 12:20:08,887 - INFO - Epoch 1 completed in 0.46s
2025-08-14 12:20:08,912 - INFO - Batch 0/80, Loss: 4.7472, LR: 0.006966
2025-08-14 12:20:08,966 - INFO - Batch 10/80, Loss: 4.9405, LR: 0.006759
2025-08-14 12:20:09,020 - INFO - Batch 20/80, Loss: 4.5227, LR: 0.006570
2025-08-14 12:20:09,074 - INFO - Batch 30/80, Loss: 4.6101, LR: 0.006396
2025-08-14 12:20:09,128 - INFO - Batch 40/80, Loss: 4.0704, LR: 0.006234
2025-08-14 12:20:09,182 - INFO - Batch 50/80, Loss: 3.6850, LR: 0.006085
2025-08-14 12:20:09,235 - INFO - Batch 60/80, Loss: 4.3772, LR: 0.005946
2025-08-14 12:20:09,289 - INFO - Batch 70/80, Loss: 4.6823, LR: 0.005816
2025-08-14 12:20:09,410 - INFO - Saved checkpoint at epoch 2
2025-08-14 12:20:09,410 - INFO - Epoch 2 completed in 0.52s
2025-08-14 12:20:09,438 - INFO - Batch 0/80, Loss: 3.3755, LR: 0.005694
2025-08-14 12:20:09,494 - INFO - Batch 10/80, Loss: 3.6275, LR: 0.005579
2025-08-14 12:20:09,548 - INFO - Batch 20/80, Loss: 3.9194, LR: 0.005471
2025-08-14 12:20:09,602 - INFO - Batch 30/80, Loss: 3.9001, LR: 0.005369
2025-08-14 12:20:09,656 - INFO - Batch 40/80, Loss: 3.4978, LR: 0.005273
2025-08-14 12:20:09,710 - INFO - Batch 50/80, Loss: 3.8672, LR: 0.005181
2025-08-14 12:20:09,764 - INFO - Batch 60/80, Loss: 3.8067, LR: 0.005095
2025-08-14 12:20:09,819 - INFO - Batch 70/80, Loss: 4.0262, LR: 0.005012
2025-08-14 12:20:09,876 - INFO - Epoch 3 completed in 0.47s
2025-08-14 12:20:09,902 - INFO - Batch 0/80, Loss: 2.9678, LR: 0.004933
2025-08-14 12:20:09,957 - INFO - Batch 10/80, Loss: 3.6080, LR: 0.004858
2025-08-14 12:20:10,010 - INFO - Batch 20/80, Loss: 3.4001, LR: 0.004786
2025-08-14 12:20:10,063 - INFO - Batch 30/80, Loss: 3.6736, LR: 0.004718
2025-08-14 12:20:10,116 - INFO - Batch 40/80, Loss: 3.7693, LR: 0.004652
2025-08-14 12:20:10,170 - INFO - Batch 50/80, Loss: 3.9377, LR: 0.004589
2025-08-14 12:20:10,223 - INFO - Batch 60/80, Loss: 3.1738, LR: 0.004528
2025-08-14 12:20:10,276 - INFO - Batch 70/80, Loss: 2.7418, LR: 0.004470
2025-08-14 12:20:10,396 - INFO - Saved checkpoint at epoch 4
2025-08-14 12:20:10,396 - INFO - Epoch 4 completed in 0.52s
2025-08-14 12:20:10,396 - INFO - Training completed in 3.34s
2025-08-14 12:20:10,503 - INFO - Saved checkpoint at epoch 5
2025-08-14 12:20:10,504 - INFO - Training finished!
2025-08-14 12:21:47,829 - INFO - Starting transformer training...
2025-08-14 12:21:47,830 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 1024,
    "dropout": 0.1,
    "src_vocab_size": 5000,
    "tgt_vocab_size": 5000
  },
  "training": {
    "batch_size": 4,
    "epochs": 5,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 100,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 2,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 128,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "auto",
    "num_workers": 2,
    "pin_memory": false
  },
  "evaluation": {
    "eval_frequency": 5,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-14 12:21:48,066 - INFO - Using device: cuda
2025-08-14 12:21:48,067 - INFO - Train texts: 1, Validation texts: 1
2025-08-14 12:21:48,228 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-14 12:21:48,886 - INFO - Batch 0/80, Loss: 8.3896, LR: 0.000088
2025-08-14 12:21:48,942 - INFO - Batch 10/80, Loss: 7.4193, LR: 0.000972
2025-08-14 12:21:48,994 - INFO - Batch 20/80, Loss: 6.5829, LR: 0.001856
2025-08-14 12:21:49,047 - INFO - Batch 30/80, Loss: 6.3937, LR: 0.002740
2025-08-14 12:21:49,100 - INFO - Batch 40/80, Loss: 6.5426, LR: 0.003624
2025-08-14 12:21:49,152 - INFO - Batch 50/80, Loss: 6.2437, LR: 0.004508
2025-08-14 12:21:49,205 - INFO - Batch 60/80, Loss: 6.1285, LR: 0.005392
2025-08-14 12:21:49,257 - INFO - Batch 70/80, Loss: 5.5057, LR: 0.006276
2025-08-14 12:21:49,492 - INFO - Epoch 0/5:
2025-08-14 12:21:49,492 - INFO -   Train Loss: 6.5879
2025-08-14 12:21:49,492 - INFO -   Val Loss: 5.5033
2025-08-14 12:21:49,493 - INFO -   Perplexity: 245.50
2025-08-14 12:21:49,557 - INFO - Saved checkpoint at epoch 0
2025-08-14 12:21:49,928 - INFO - Sample: the foundation , for own , and leafy , and acquaintances of the singers you whenever bold , and in april . it is 97 gutenberg ' s of the ground ™ . chi , and times to with frequently , , - - lights , ...
2025-08-14 12:21:49,993 - INFO - Saved checkpoint at epoch 0
2025-08-14 12:21:49,993 - INFO - Epoch 0 completed in 1.29s
2025-08-14 12:21:50,021 - INFO - Batch 0/80, Loss: 5.3236, LR: 0.007159
2025-08-14 12:21:50,076 - INFO - Batch 10/80, Loss: 5.6803, LR: 0.008043
2025-08-14 12:21:50,128 - INFO - Batch 20/80, Loss: 5.8663, LR: 0.008795
2025-08-14 12:21:50,181 - INFO - Batch 30/80, Loss: 5.3661, LR: 0.008389
2025-08-14 12:21:50,233 - INFO - Batch 40/80, Loss: 5.3982, LR: 0.008035
2025-08-14 12:21:50,285 - INFO - Batch 50/80, Loss: 5.3350, LR: 0.007723
2025-08-14 12:21:50,338 - INFO - Batch 60/80, Loss: 5.2112, LR: 0.007444
2025-08-14 12:21:50,390 - INFO - Batch 70/80, Loss: 5.2770, LR: 0.007193
2025-08-14 12:21:50,445 - INFO - Epoch 1 completed in 0.45s
2025-08-14 12:21:50,471 - INFO - Batch 0/80, Loss: 4.7472, LR: 0.006966
2025-08-14 12:21:50,525 - INFO - Batch 10/80, Loss: 4.9405, LR: 0.006759
2025-08-14 12:21:50,583 - INFO - Batch 20/80, Loss: 4.5227, LR: 0.006570
2025-08-14 12:21:50,635 - INFO - Batch 30/80, Loss: 4.6101, LR: 0.006396
2025-08-14 12:21:50,688 - INFO - Batch 40/80, Loss: 4.0704, LR: 0.006234
2025-08-14 12:21:50,743 - INFO - Batch 50/80, Loss: 3.6850, LR: 0.006085
2025-08-14 12:21:50,795 - INFO - Batch 60/80, Loss: 4.3772, LR: 0.005946
2025-08-14 12:21:50,848 - INFO - Batch 70/80, Loss: 4.6823, LR: 0.005816
2025-08-14 12:21:50,966 - INFO - Saved checkpoint at epoch 2
2025-08-14 12:21:50,966 - INFO - Epoch 2 completed in 0.52s
2025-08-14 12:21:50,995 - INFO - Batch 0/80, Loss: 3.3755, LR: 0.005694
2025-08-14 12:21:51,047 - INFO - Batch 10/80, Loss: 3.6275, LR: 0.005579
2025-08-14 12:21:51,100 - INFO - Batch 20/80, Loss: 3.9194, LR: 0.005471
2025-08-14 12:21:51,152 - INFO - Batch 30/80, Loss: 3.9001, LR: 0.005369
2025-08-14 12:21:51,203 - INFO - Batch 40/80, Loss: 3.4978, LR: 0.005273
2025-08-14 12:21:51,255 - INFO - Batch 50/80, Loss: 3.8672, LR: 0.005181
2025-08-14 12:21:51,307 - INFO - Batch 60/80, Loss: 3.8067, LR: 0.005095
2025-08-14 12:21:51,360 - INFO - Batch 70/80, Loss: 4.0262, LR: 0.005012
2025-08-14 12:21:51,416 - INFO - Epoch 3 completed in 0.45s
2025-08-14 12:21:51,443 - INFO - Batch 0/80, Loss: 2.9678, LR: 0.004933
2025-08-14 12:21:51,496 - INFO - Batch 10/80, Loss: 3.6080, LR: 0.004858
2025-08-14 12:21:51,550 - INFO - Batch 20/80, Loss: 3.4001, LR: 0.004786
2025-08-14 12:21:51,604 - INFO - Batch 30/80, Loss: 3.6736, LR: 0.004718
2025-08-14 12:21:51,657 - INFO - Batch 40/80, Loss: 3.7693, LR: 0.004652
2025-08-14 12:21:51,712 - INFO - Batch 50/80, Loss: 3.9377, LR: 0.004589
2025-08-14 12:21:51,766 - INFO - Batch 60/80, Loss: 3.1738, LR: 0.004528
2025-08-14 12:21:51,819 - INFO - Batch 70/80, Loss: 2.7418, LR: 0.004470
2025-08-14 12:21:51,940 - INFO - Saved checkpoint at epoch 4
2025-08-14 12:21:51,940 - INFO - Epoch 4 completed in 0.52s
2025-08-14 12:21:51,940 - INFO - Training completed in 3.24s
2025-08-14 12:21:52,157 - INFO - Saved checkpoint at epoch 5
2025-08-14 12:21:52,158 - INFO - Training finished!
2025-08-14 12:26:19,852 - INFO - Starting transformer training...
2025-08-14 12:26:19,853 - INFO - Configuration: {
  "model": {
    "d_model": 128,
    "num_heads": 4,
    "num_layers": 2,
    "d_ff": 512,
    "max_len": 1024,
    "dropout": 0.1,
    "src_vocab_size": 5000,
    "tgt_vocab_size": 5000
  },
  "training": {
    "batch_size": 4,
    "epochs": 5,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 100,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 2,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 128,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "auto",
    "num_workers": 2,
    "pin_memory": false
  },
  "evaluation": {
    "eval_frequency": 5,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-14 12:26:20,000 - INFO - Using device: cuda
2025-08-14 12:26:20,001 - INFO - Train texts: 1, Validation texts: 1
2025-08-14 12:26:20,142 - INFO - Model parameters: 2,313,242 total, 2,313,242 trainable
2025-08-14 12:26:20,801 - INFO - Batch 0/80, Loss: 8.3896, LR: 0.000088
2025-08-14 12:26:20,858 - INFO - Batch 10/80, Loss: 7.4193, LR: 0.000972
2025-08-14 12:26:20,911 - INFO - Batch 20/80, Loss: 6.5829, LR: 0.001856
2025-08-14 12:26:20,964 - INFO - Batch 30/80, Loss: 6.3937, LR: 0.002740
2025-08-14 12:26:21,017 - INFO - Batch 40/80, Loss: 6.5426, LR: 0.003624
2025-08-14 12:26:21,070 - INFO - Batch 50/80, Loss: 6.2437, LR: 0.004508
2025-08-14 12:26:21,126 - INFO - Batch 60/80, Loss: 6.1285, LR: 0.005392
2025-08-14 12:26:21,179 - INFO - Batch 70/80, Loss: 5.5057, LR: 0.006276
2025-08-14 12:26:21,414 - INFO - Epoch 0/5:
2025-08-14 12:26:21,414 - INFO -   Train Loss: 6.5879
2025-08-14 12:26:21,414 - INFO -   Val Loss: 5.5033
2025-08-14 12:26:21,414 - INFO -   Perplexity: 245.50
2025-08-14 12:26:21,480 - INFO - Saved checkpoint at epoch 0
2025-08-14 12:26:21,861 - INFO - Sample: the foundation , for own , and leafy , and acquaintances of the singers you whenever bold , and in april . it is 97 gutenberg ' s of the ground ™ . chi , and times to with frequently , , - - lights , ...
2025-08-14 12:26:21,926 - INFO - Saved checkpoint at epoch 0
2025-08-14 12:26:21,926 - INFO - Epoch 0 completed in 1.31s
2025-08-14 12:26:21,953 - INFO - Batch 0/80, Loss: 5.3236, LR: 0.007159
2025-08-14 12:26:22,006 - INFO - Batch 10/80, Loss: 5.6803, LR: 0.008043
2025-08-14 12:26:22,059 - INFO - Batch 20/80, Loss: 5.8663, LR: 0.008795
2025-08-14 12:26:22,113 - INFO - Batch 30/80, Loss: 5.3661, LR: 0.008389
2025-08-14 12:26:22,167 - INFO - Batch 40/80, Loss: 5.3982, LR: 0.008035
2025-08-14 12:26:22,219 - INFO - Batch 50/80, Loss: 5.3350, LR: 0.007723
2025-08-14 12:26:22,270 - INFO - Batch 60/80, Loss: 5.2112, LR: 0.007444
2025-08-14 12:26:22,323 - INFO - Batch 70/80, Loss: 5.2770, LR: 0.007193
2025-08-14 12:26:22,379 - INFO - Epoch 1 completed in 0.45s
2025-08-14 12:26:22,404 - INFO - Batch 0/80, Loss: 4.7472, LR: 0.006966
2025-08-14 12:26:22,457 - INFO - Batch 10/80, Loss: 4.9405, LR: 0.006759
2025-08-14 12:26:22,510 - INFO - Batch 20/80, Loss: 4.5227, LR: 0.006570
2025-08-14 12:26:22,564 - INFO - Batch 30/80, Loss: 4.6101, LR: 0.006396
2025-08-14 12:26:22,617 - INFO - Batch 40/80, Loss: 4.0704, LR: 0.006234
2025-08-14 12:26:22,669 - INFO - Batch 50/80, Loss: 3.6850, LR: 0.006085
2025-08-14 12:26:22,722 - INFO - Batch 60/80, Loss: 4.3772, LR: 0.005946
2025-08-14 12:26:22,774 - INFO - Batch 70/80, Loss: 4.6823, LR: 0.005816
2025-08-14 12:26:22,896 - INFO - Saved checkpoint at epoch 2
2025-08-14 12:26:22,896 - INFO - Epoch 2 completed in 0.52s
2025-08-14 12:26:22,923 - INFO - Batch 0/80, Loss: 3.3755, LR: 0.005694
2025-08-14 12:26:22,977 - INFO - Batch 10/80, Loss: 3.6275, LR: 0.005579
2025-08-14 12:26:23,030 - INFO - Batch 20/80, Loss: 3.9194, LR: 0.005471
2025-08-14 12:26:23,083 - INFO - Batch 30/80, Loss: 3.9001, LR: 0.005369
2025-08-14 12:26:23,138 - INFO - Batch 40/80, Loss: 3.4978, LR: 0.005273
2025-08-14 12:26:23,191 - INFO - Batch 50/80, Loss: 3.8672, LR: 0.005181
2025-08-14 12:26:23,243 - INFO - Batch 60/80, Loss: 3.8067, LR: 0.005095
2025-08-14 12:26:23,295 - INFO - Batch 70/80, Loss: 4.0262, LR: 0.005012
2025-08-14 12:26:23,350 - INFO - Epoch 3 completed in 0.45s
2025-08-14 12:26:23,376 - INFO - Batch 0/80, Loss: 2.9678, LR: 0.004933
2025-08-14 12:26:23,429 - INFO - Batch 10/80, Loss: 3.6080, LR: 0.004858
2025-08-14 12:26:23,481 - INFO - Batch 20/80, Loss: 3.4001, LR: 0.004786
2025-08-14 12:26:23,533 - INFO - Batch 30/80, Loss: 3.6736, LR: 0.004718
2025-08-14 12:26:23,584 - INFO - Batch 40/80, Loss: 3.7693, LR: 0.004652
2025-08-14 12:26:23,637 - INFO - Batch 50/80, Loss: 3.9377, LR: 0.004589
2025-08-14 12:26:23,689 - INFO - Batch 60/80, Loss: 3.1738, LR: 0.004528
2025-08-14 12:26:23,743 - INFO - Batch 70/80, Loss: 2.7418, LR: 0.004470
2025-08-14 12:26:23,861 - INFO - Saved checkpoint at epoch 4
2025-08-14 12:26:23,861 - INFO - Epoch 4 completed in 0.51s
2025-08-14 12:26:23,861 - INFO - Training completed in 3.24s
2025-08-14 12:26:23,971 - INFO - Saved checkpoint at epoch 5
2025-08-14 12:26:23,972 - INFO - Training finished!
2025-08-14 12:29:06,764 - INFO - Starting transformer training...
2025-08-14 12:29:06,764 - INFO - Configuration: {
  "model": {
    "d_model": 256,
    "num_heads": 8,
    "num_layers": 4,
    "d_ff": 1024,
    "max_len": 512,
    "dropout": 0.1,
    "src_vocab_size": 10000,
    "tgt_vocab_size": 10000
  },
  "training": {
    "batch_size": 4,
    "epochs": 3,
    "learning_rate": 0.001,
    "scheduler": {
      "type": "warmup",
      "warmup_steps": 4000,
      "step_factor": 0.5,
      "step_size": 10
    },
    "grad_clip": 1.0,
    "weight_decay": 0.01,
    "save_frequency": 1,
    "log_frequency": 10
  },
  "data": {
    "cache_dir": "text_cache",
    "sequence_length": 512,
    "min_text_length": 1000,
    "train_split": 0.9,
    "max_files": null,
    "special_tokens": {
      "pad_token": "<PAD>",
      "unk_token": "<UNK>",
      "start_token": "<START>",
      "end_token": "<END>"
    }
  },
  "output": {
    "checkpoint_dir": "output/checkpoints",
    "log_dir": "output/logs",
    "samples_dir": "output/samples",
    "model_name": "transformer_lm"
  },
  "generation": {
    "max_length": 200,
    "temperature": 0.8,
    "top_k": 50,
    "top_p": 0.9,
    "num_samples": 5
  },
  "hardware": {
    "device": "auto",
    "num_workers": 2,
    "pin_memory": true
  },
  "evaluation": {
    "eval_frequency": 1,
    "metrics": [
      "perplexity",
      "loss"
    ],
    "eval_samples": 1000
  }
}
2025-08-14 12:29:06,910 - INFO - Using device: cuda
2025-08-14 12:29:06,911 - INFO - Train texts: 1, Validation texts: 1
2025-08-14 12:29:07,097 - INFO - Model parameters: 10,139,674 total, 10,139,674 trainable
2025-08-14 12:29:07,763 - INFO - Batch 0/20, Loss: 8.2903, LR: 0.000000
2025-08-14 12:29:07,872 - INFO - Batch 10/20, Loss: 7.6486, LR: 0.000003
2025-08-14 12:29:08,071 - INFO - Epoch 0/3:
2025-08-14 12:29:08,071 - INFO -   Train Loss: 7.7148
2025-08-14 12:29:08,071 - INFO -   Val Loss: 7.6127
2025-08-14 12:29:08,071 - INFO -   Perplexity: 2023.65
2025-08-14 12:29:08,363 - INFO - Saved checkpoint at epoch 0
2025-08-14 12:29:08,888 - INFO - Sample: the illustrated chance internal flutter synonymous scissor indicating agents successful kerosene luxurious the copied gannett offer _stenographic believe curved sole instinct transcriber confined subj...
2025-08-14 12:29:09,227 - INFO - Saved checkpoint at epoch 0
2025-08-14 12:29:09,227 - INFO - Epoch 0 completed in 1.65s
2025-08-14 12:29:09,260 - INFO - Batch 0/20, Loss: 7.7672, LR: 0.000005
2025-08-14 12:29:09,368 - INFO - Batch 10/20, Loss: 7.6504, LR: 0.000008
2025-08-14 12:29:09,569 - INFO - Epoch 1/3:
2025-08-14 12:29:09,569 - INFO -   Train Loss: 7.5883
2025-08-14 12:29:09,569 - INFO -   Val Loss: 7.5112
2025-08-14 12:29:09,569 - INFO -   Perplexity: 1828.37
2025-08-14 12:29:09,880 - INFO - Saved checkpoint at epoch 1
2025-08-14 12:29:10,304 - INFO - Sample: the flute university inhabitants getting delivered machine posted silver_ indexed , the , singing could know songsters written maintains twitter benders severe live moths xvii positions _department in...
2025-08-14 12:29:10,750 - INFO - Saved checkpoint at epoch 1
2025-08-14 12:29:10,750 - INFO - Epoch 1 completed in 1.52s
2025-08-14 12:29:10,782 - INFO - Batch 0/20, Loss: 7.5013, LR: 0.000010
2025-08-14 12:29:10,891 - INFO - Batch 10/20, Loss: 7.5459, LR: 0.000013
2025-08-14 12:29:11,091 - INFO - Epoch 2/3:
2025-08-14 12:29:11,091 - INFO -   Train Loss: 7.4713
2025-08-14 12:29:11,091 - INFO -   Val Loss: 7.3682
2025-08-14 12:29:11,091 - INFO -   Perplexity: 1584.83
2025-08-14 12:29:11,474 - INFO - Saved checkpoint at epoch 2
2025-08-14 12:29:11,916 - INFO - Sample: the attractive 750 advancement contain , sits boys introducers plate deal this encourage xxiii ecstatic washington make solid strongly exempt definition upper , nests aquatic cheapest mothers hunted n...
2025-08-14 12:29:12,295 - INFO - Saved checkpoint at epoch 2
2025-08-14 12:29:12,296 - INFO - Epoch 2 completed in 1.54s
2025-08-14 12:29:12,296 - INFO - Training completed in 4.72s
2025-08-14 12:29:12,584 - INFO - Saved checkpoint at epoch 3
2025-08-14 12:29:12,584 - INFO - Training finished!
